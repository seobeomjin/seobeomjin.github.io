---
layout : post 
date : 2020-10-16
title :  Record of Standford CS234 Course, Reinforcement
description : Record of Standford CS234 Course, Reinforcement
published : False
--- 

Record of Standford CS234 Course, Reinforcement

#### Lecture 01 
    2020-10-16 <br>
* keyword <br>
    - RL involves <br>
        Optimization / Delayed consquences / Exploration / Generalization 
    - Imitation Learning <br>
    - Sequential Decision Process (under uncertainity)<br>
        Markov Assumption <br>
        MDPs / POMDPs(Partially Observable) / Bandits <br>
        How the World Changes -> Determisinstic / Stochastic<br>
    - RL Algoritm Components <br>
        Model / Policy / Value function  <br>
    - Types of RL Agents <br>
        Model-based / Model-free / Valued-based / Policy-based / Actor-Critic<br>
    - Key Challenges in Learning to Make Sequences of Good Decisions <br>
        Planning / Reinforcement learning <br>
    - Exploration and Exploitation <br>

* Summary Questions <br>
    1. What is the Markov Assumption? <br>
    2. Why is Markov Assumption Popluar? 

#### Lecture 02 
    2020-10-27 - (take 1h, till 20min of whole video)<br>
    2020-11-07 - (take 50min, till 29min of whole video) <br>

* keyword <br>
    - Markov Chain  <br>
        sequence of random states with Markov property 
    - Markov Processes  <br>
        S is a (finite) set of states <br> 
        P is dynamics/transition models that specifieces p(st+1 = s' | st = s)
        No rewards, No actions 
    - Markov Reward Processes (MRPs) <br>
        S , P 
        R is a reward function R (st=s) = E[rt|st=s]
        Discont factor r in [0,1]
        - Computing the Value of a MRP 
            1. estimating by simulation 
                MRP vlaue function  : V(s) = R(s) + r*sum(P(s'|s)V(s'))
            2. analytical way 
                expression with matricies , V = (1-rP)^-1*R
            3. Dynamic Programming 
                V0(s) = 0 for all s , for k = 1 until convergence 
                    for all s in S 
                        Vk(s) = R(s) + r*sum(P(s'|s)Vk-1(s'))
    - Evaluation and Control in MDPs 



* Summary Questions <br>
    1. what is a Markov Chain? <br>
    2. Definition of Markov Process <br>
    3. Definition of Markov Reward Process (MRP) <br>
    4. what are meanings of Horizon, Return, and Value Function for a MRP? <br>
    5. How to calculate a Vaule of a MRP ? (3 methods)
    

