---
layout : post
date : 2020-10-15
title : Record of Standford CS224N Course, Natural Language Processing
description: Record of Standford CS224N Course, Natural Language Processing
published : False
---

Record of Standford CS224N Course, Natural Language Processing

#### Lecture 01
2020-10-15 
* keyword 
    - denotational sementics 
    - WordNet <br>
        X -> missing naunce, missing new meanings of words 
    - discrete symbols <br>
        naunce, too big size, no similarity 
    - distributed sementics 
    - distributed representation <br>
        It is sometimes called such as word vectors, word embeddings or word representation. 
    - Word2Vec <br>
        Overview(centor words, context words), Likelihood, Objective function, prediction function, how to optimize
* Summary Questions <br>
    1. write the Likelihood L(theta) of Word2Vec and describe meaning of it briefly.<br>
    2. write the objective function J(theta) of Word2Vec and describe meaning of it briefly. <br>
    3. write the prediction function p of Word2Vec and describe meaning of it briefly. <br>
    4. why "soft" and "max" ?<br>
    5. write the process of derivate of objective function and meaning of it briefly. <br>


#### Lecture 02
2020-11-07 (-26min, take 60mins)
* keyword 
    - SGD Issue of Word2Vec 


* Summary Questions 
    1. write the equation of Gradient Descent. 
    2. Why do we use Stochastic Gradient Descent ? 
    3. EX) vision의 SGD와 다르게, 언어모델에서 SGD를 활용할 때 발생할 수 있는 문제점이 무엇인지 서술하시오. 