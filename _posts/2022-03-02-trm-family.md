---
layout: post
title: The Transformer Family
subtitle: Short Summary about The Transformer Family
categories: Study
tags: transformer
comments: True
published: True
---

## Vanila Transformer
![vanila trm](/assets/images/trm-family/vanila-trm.png)<br>
- self-attention is applied in each encoder and decoer. cross-attention  is applied between encoder and decoder. <br>
- dot(Query vector, Key vector) =  attention score. <br>
- and then dot(attention score, Value vector) = attention value. <br>
- No long term dependency  

## Linformer 
![linformer](/assets/images/trm-family/linformer.jpg) <br>





## Reference 
- https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/
- Linformer