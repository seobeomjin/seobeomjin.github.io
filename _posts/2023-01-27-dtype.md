---
layout: post
title: FP16, FP32, BF16, Mixed Precision 
subtitle: training에 사용되는 data type에 관한 내용을 정리합니다. 
categories: Study
tags: nlp data-type floating-point
comments: True
published: True
---

- computer에서 실수 표현하기 
    - 실수는 수의 범위가 무한하기 때문에, 이를 bit로 정확하게 표현하는 것은 한계가 있다. <br>
      이를 표현하기 위해 floating point (부동소수점) 를 활용 <br>
    - sign(부호), exponent(지수), fraction(가수)<br>
        sign 부호는 1bit <br>
        exponent 는 실수의 정수를 나타내는 부분으로, 이 부분의 bit가 크면 더 큰 범위의 수를 표현할 수 있음. <br>
        Mantissa (francion) 은 실수의 소수를 나타내는 부분으로, 이 부분의 bit가 크면 더 정확하게 실수를 표현할 수 있음.<br>
    - 어떤 실수를 정규형으로 표현하면, <br>
        N = (-1)^s * M * 2^E 임. <br>
        M = 1.xxxxxxxxx 에서 xxxxxxxx 가 0또는 1로 표현되고, 이 값이 순서대로 Mantissa 부분에 입력됨. <br>
        2^E 에서의 E가 지수 부분을 표현하며, bias+E = value -> 이 value를 2진법으로 표현한 값을 입력 (여기서 bias = 2^(k-1), k=지수부의 bit수)<br>
    - 표현형에 따라 overflow 문제가 발생 <br>
        overflow는 계산 결과가 표현 가능한 허용 범위를 초과한 값이 나올 때 발생한다. 예를 들어 divide-by-zero 는 매우 큰 값을 가져 overflow 발생 <br>
        underflow는 실수가 0에 가까울 정도로 매우 작으면 underflow가 발생한다. 파이썬은 이 값을 0으로 근사시킴. <br>

- fp16, fp32, bf16, mixed precision <br>
    - fp16 : half precision <br>
        <!-- 표현범위가 [-2^7, 2^7].  -->
        exponent (이진수 표현) 와 fraction (2의지수) 의 표현 범위가 그닥 크지 못함. <br>
    - fp32 : single precision <br>
        <!-- 표현범위 [-2^14, 2^14] -->
        exponent (이진수 표현) 와 fraction (2의지수) 의 정밀도가 fp16에 비해 큼. <br>
    - bf16 <br>
        - int16 or fp16은 표현형이 fp32에 비해 작아서, forward or backward process 에서 under/overflow 발생을 막기 위해 scailing 을 해야 함. <br>
        - bf16의 표현범위는 fp32와 같음. (exponent 8bit, fraction 7bit 줌) 단, Mantissa 부분의 bit가 적어서, fp32에 비해서는 정확하게 표현하지 못함. <br>
    - mixed precision <br>
        - fp16과 fp32를 type casting 해가며 model을 학습하는 방법. <br>
        - amp library (auto mixed precision) 가 이것임. 더욱 빠르고, 정확하게(?) 학습할 수 있음. <br>
        - DNN에서 weight, weight gradients, activation, activation gradients 가 적용되는 tensor임. 이 과정에서 type casting으로 학습을 빠르게, 정확하게 하려함.<br>
        - 다만, fp32에서 type casting을 하는 master object 에 bottleneck 이 걸리는 단점이 있다???  <br>

- 장단점이 있음 <br>
    - fp16을 쓰면, memory를 적게 먹고, 그 만큼 연산이 빠른데 비해 성능이 떨어짐. <br>
    - fp32 는, memory를 더 쓰고, 연산이 더 걸리지만, 성능이 보장됨. <br>
    - 표현형의 정밀도가 떨어지면, gradient 값이 표현형의 제약으로 zeroing 이 되는 값들이 많아짐 (사실상 거의 0에 근사하여 0값을 가지게 만드는?) (NVIDIA bf16 related post 참고)<br>

### Reference 
- mixed precision (https://velog.io/@twinjuy/Auto-Mixed-Precision%EC%9D%B4%EB%9E%80)
- floating point (https://velog.io/@mjk3136/%EB%B6%80%EB%8F%99%EC%86%8C%EC%88%98%EC%A0%90%EC%97%90-%EB%8C%80%ED%95%B4-%EC%95%8C%EA%B8%B0)
- bf16 (논문리뷰 A Study of BFLOAT16 for Deep Learning Training)