---
layout: post
title: ViLBERT paper review
subtitle: ViLBERT, Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks
categories: Paper-review
tags: multi-modal-learning self-supervised-learning pretrain
comments: true
published: True
---

## Summary 
본 논문에서는 Image 와 Text modality를 함께 학습할 수 있는 multimodal model architecture 인 <strong>ViLBERT(Vision-and-Language BERT)</strong>를 제안합니다. 아이디어를 간단히 소개하자면, visual input과 textual input 이 각각의 stream으로 입력되고, co-attentional transformer layer를 통해 서로 다른 modality 사이에서 interaction하며 학습합니다. ViLBERT는 [Conceptual Captions dataset](https://ai.google.com/research/ConceptualCaptions/) 기반의 2가지의 self-supervised learning 을 통해 pretraining을 수행하고, vision-and-language task들(visual question answering, visual commonsense reasoning, referring expressions, and caption-based image retrieval) 에 대해 transfer learning을 수행합니다. 위의 task들에서 State-Of-The-Art 성능을 보여주면서 pretrainable 하며, transferable capability를 보여주고 있습니다.
## Main Idea
![model architecture](/assets/images/vilbert/fig1.jpg) 
<br>
ViLBERT의 주요한 아이디어는 co-attentional transformer layer를 통해 서로 다른 modality 사이에 interative 하게 학습이 가능한 점입니다. 위의 그림과 같이 Image 와 Text 가 2 stream으로 입력되어, co-attentional transformer 를 통해 학습합니다. (TRM은 Transformer를 약자로 쓴 것입니다.) <br>
<br>
ViLBERT는 Image를 직접 입력으로 받는 것이 아니라, pretrained Faster R-CNN 모델을 통해 region feature를 추출하고, 해당 feature들을 사용합니다. 여기에 사용되는 region feature들은 class detecion probability가 confidence threshold가 넘어야 선별되며, 위의 그림에서 $v_i$ 는 이렇게 선별된 region의 mean-pooled convolutional feature를 나타냅니다. Text를 전처리하는 방법은 BERT에서의 방법과 동일합니다. 

![fig](/assets/images/vilbert/fig2.jpg)
<br>
위의 그림의 (a)는 기존의 Transformer encoder 를 나타내고, (b)는 ViLBERT의 co-attentional transformer layer를 보여줍니다. 
![fig](/assets/images/vilbert/fig3.jpg)
<br>
## Experiments 
![fig](/assets/images/vilbert/fig4.jpg)
<br>
![fig](/assets/images/vilbert/fig5.jpg)
<br>
## Reference
- <a href="https://arxiv.org/abs/1908.02265"> Paper </a><br>
<!-- ## 느낀 점 -->


<!-- ## Main Contribution 
## Introduction
## Method
## Experiments 
## Conclusion  -->
<!-- - main contribution 
- intro
- method 
- exp 
- conclusion
- 느낀 점 & 논문 읽으며 궁금했던 점, 배운점  -->
