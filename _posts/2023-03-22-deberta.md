---
layout: post
title: \[논문리뷰] DeBERTa, Decoding-enhanced BERT with Disentangled Attention
subtitle: Disentangled Attention, Enhanced Mask Decoder를 활용한 DeBERTa 논문을 리뷰합니다. 
categories: Paper-review  # [Paper-review, Study] 
tags: nlp bert attention
comments: True
published: True
---

# Main Contribution
- Disentangled Attention <br> 
    hidden vector 를 contents를 포함하는 vector와 position 을 포함하는 vector로 나눈다. <br>
    attention score를 얻을 때, content-to-content, content-to-position, position-to-content 의 element-wise summation 을 통해 attention score 를 얻는다. <br>
- Enhanced Mask Decoder (EMD)<br>
    relative positional information을 활용되었지만, absolute positional information은 활용되지 않았다. <br>
    - ex <br>
        "a new store opened beside the new mall" 이라는 문장에서 store 와 mall 이라는 단어가 masking이 되었을 때, <br>
        두 단어 모두 앞에 new라는 단어의 등장으로, 어떤 단어가 해당 위치에 와야 하는지 판별하기 어렵다고 논문에서는 주장한다. <br>
        이에 absolute positional information을 Transforer layer 이후, softmax 를 지나기 전 포함시킨다. <br>

# Reference
- <a href="https://arxiv.org/abs/2006.03654"> paper </a><br>