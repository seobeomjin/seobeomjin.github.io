---
layout: post
title: GPT-1 논문리뷰 
subtitle: GPT; Improving Language Understanding by Generative Pre-Training 논문 리뷰 
categories: Paper-review
tags: language-model nlp paper gpt
comments: True
published: True
---


### GPT1

- **Abstract**
    - unlabeled data 는 labeled data 대비 양이 방대하게 많고, 적은 양의 labeled data 로 task-specific 한 모델들을 각각 만드는 것은 어렵다. 
    - **generative pre-training** 한 방법을 활용해 모델을 학습시키고, 이 후 **discriminative fine-tuning** 의 approach를 제안한다. 
    - 이 방법은 a wide range of benchmarks for natural language understanding 에서 outperformance 를 이뤘다. 
        - state of the art in 9 out of the 12 tasks studied.
        - e.g. absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI).

- **Motiviation**  
    - unlabeled data 는 labeled data 에 비해서 훨씬 그 양이 많다. 
    - 따라서 unlabeled data 를 활용하여 언어모델을 pre-training 하고, labeled text corpora 를 통해 specific task 를 fine-tuning 해보자. 

- **Unlabeled Data 활용의 문제점**
    - Leveraging more than word-level information from unlabeled text 는 challenging 하다. (2가지 이유 때문)
      <!-- (단어 수준 이상의 정보를 뽑아내는 것이 challenging 하다?) -->
        1. 최적화 objective 를 어떻게 설정하는 것이 transfer에 유용한 text representation 을 학습하는 것인지 불분명하다. 
        2. these learned representations 을 target task 로 transfer 하기 위한, the most effecitve way에 대한 consensus 가 없다. 
            - 기존의 방법들은, 
                - a combination of making task-specific changes to the model architecture using intricate learning schemes and adding auxiliary learning objectives.
- **Method** 
    - a semi-supervised approach for language understanding tasks 
        - using **a combination of unsupervised pre-training and supervised fine-tuning.**
        - unsupervised learnig 즉, generative 하게 문장을 만드는 방법으로 pretrinaing 을 수행하고, fine-tuning 을 수행할 때는 task specific data와 해당 label을 맞추는 확률을 극대화 하도록 하는 objective 를 활용한다. 
    - Model Architecture <br>
        ![img](/assets/images/gpt1/gpt1_model.jpg)
        - Transformer decoder 블록만을 활용하고, 그중에서도 encoder는 안쓰기 때문에 cross-attention 이 활용되는 부분은 없다. masked multi-head attention 으로 이루어진 decoder 블록들을 활용한다. 
    - two stage 로 진행 <br>
        1. Unsupervised pre-training <br>
            ![img](/assets/images/gpt1/gpt1_unsup.jpg)
            - 모델 파라미터와 이전의 context window k 만큼의 token 이 주어졌을 때, i번째 token이 발생할 likelihood를 maximize 하도록 학습한다. 
            - L(U) 는 unlabeled data에 대한 loss 발생을 의미한다.
        2. Supervised fine-tuning <br>
            ![img](/assets/images/gpt1/gpt1_sup.jpg)
            - y값을 낼 때에는, final transformer block 과 linear output layer, softmax 를 활용하여 그 값을 얻는다. 또한 그 값을 likelihood maximize 방법으로 학습하도록 한다. 
            - L(C) 는 labeled data에 대해 발생하는 loss를 의미한다. 

- **Including language modeling as an auxilary objective to the fine-tuning 의 이점** 
    <!-- aux obj(sub task) 를 말하는데, 여기서는 sup objective 를 말하는?-->
    1. supervised model의 일반화 성능 향상 
    2. convergence 속도 accerlation 

- **Experiments**
    - 각 task specific transformer model architecture <br>
        ![img](/assets/images/gpt1/trm_arch_for_tasks.jpg) 
    - Natural Lanugage Inference (textual entailment)
        - We evaluate on five datasets with diverse sources, 
            - including image captions (SNLI), transcribed speech, popular fiction, and government reports (MNLI), Wikipedia articles (QNLI), science exams (SciTail) or news articles (RTE).
        - ![img](/assets/images/gpt1/gpt1_nli.jpg) 
    - Qeustion Answering and Commonsense Reasoning 
        - datasets 
            - RACE dataset, consisting of English passages with associated questions from middle and high school exams.
            - the Story Cloze Test [40], which involves selecting the correct ending to multi-sentence stories from two options.
        - ![img](/assets/images/gpt1/gpt1_qa.jpg)  
    - Semantic Similarity & Classification 
        - ![img](/assets/images/gpt1/gpt1_cls.jpg)

- **Analaysis** <br> 
    ![img](/assets/images/gpt1/gpt1_analysis.jpg)
    - Impact of number of layers transferred & Zero-shot performance 
        - decoder 블록을 쌓을수록 성능이 높아진다. 
        - pretraining updates 가 많아지면 성능이 높아진다. (?)
    ![img](/assets/images/gpt1/gpt1_ablation.jpg)
    - auxiliary objective (subtask 등 다른 task의 objective term을 함께 학습에 사용하는 것) 
        task-specific 한 labeled dataset size가 클 때는 aux LM (task specific objective term) 이 효과가 있지만, labeled dataset size가 작을 때는 그렇게 큰 효과를 보지 못한다. 


### Reference 
- <a href="https://www.youtube.com/watch?v=o_Wl29aW5XM&list=PLetSlH8YjIfVzHuSXtG4jAC2zbEAErXWm&index=19" > 강필성 교수님 GPT review </a>
- <a href = "https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf"> paper </a>

